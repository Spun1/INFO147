{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from functools import partial\n",
    "slider_layout = widgets.Layout(width='600px', height='20px')\n",
    "slider_style = {'description_width': 'initial'}\n",
    "IntSlider_nice = partial(widgets.IntSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "SelSlider_nice = partial(widgets.SelectionSlider, style=slider_style, layout=slider_layout, continuous_update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión lineal\n",
    "\n",
    "**Regresión:** Consiste en **ajustar** un modelo paramétrico\n",
    "$$\n",
    "f_\\theta: x \\rightarrow y\n",
    "$$\n",
    "que sea capaz de predecir $y$ dado $x$\n",
    "\n",
    "- $x$ variable indepediente, entrada, característica, predictor\n",
    "- $y$ variable dependiente, salida, respuesta, objetivo (target)\n",
    "- $x$ e $y$ son variables continuas\n",
    "- $\\theta$ son los parámetros del modelo\n",
    "\n",
    "> Encontrar como dos o más variables se relacionan\n",
    "\n",
    "> Explicar/Predecir una variable en función de otras\n",
    "\n",
    "Hablamos de **regresión lineal** cuando el modelo $f_\\theta$ es **lineal en sus parámetros**\n",
    "\n",
    "- **Datos:** conjunto de $M$ tuplas $(\\vec x_i, y_i)$ con $i=1,2,\\ldots,M$ \n",
    "- **Ajuste:** Encontrar el valor óptimo de $\\theta$ en función de los datos\n",
    "- Si  $f_\\theta$ es lineal en sus parámetros se puede escribir como un **sistema lineal de $M$ ecuaciones**\n",
    "- Si el sistema es rectangular lo podemos resolver con **Mínimos Cuadrados**\n",
    "$$\n",
    "\\min_\\theta \\sum_{i=1}^M \\left(y_i - f_\\theta(\\vec x_i) \\right)^2\n",
    "$$\n",
    "- En dicho caso la solución es óptima *\"en el sentido de mínimos cuadrádos\"*\n",
    "\n",
    "### Modelos lineales en sus parámetros y en sus entradas\n",
    "#### Recta\n",
    "Si $x$ es unidimensional \n",
    "$$\n",
    "y =\\theta_0  + \\theta_1 x \n",
    "$$\n",
    "\n",
    "#### Plano\n",
    "Si $\\vec x =(x_1, x_2)$ es bidimensional \n",
    "$$\n",
    "y = \\theta_0  + \\theta_1 x_1 +  \\theta_2 x_2 \n",
    "$$\n",
    "\n",
    "#### Hiperplano\n",
    "Si $\\vec x = (x_1, x_2, \\ldots, x_d)$ es d-dimensional\n",
    "$$\n",
    "y = \\theta_0  + \\sum_{k=1}^d \\theta_k x_k \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, SelectionSlider, IntSlider\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "plt.close('all'); fig = plt.figure(figsize=(6, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "theta = [4, 3, 2]; \n",
    "\n",
    "def update(rseed, N, sigma):\n",
    "    ax.cla();\n",
    "    np.random.seed(rseed);\n",
    "    x1, x2 = np.random.randn(2, N)\n",
    "    y_clean = theta[0] + theta[1]*x1 + theta[2]*x2 \n",
    "    y = y_clean + sigma*np.random.randn(len(x1))\n",
    "    X_lstsq = np.stack((np.ones_like(x1), x1, x2)).T\n",
    "    param, MSE, rank, singval = np.linalg.lstsq(X_lstsq, y, rcond=None)\n",
    "    display(theta, param)\n",
    "    ax.scatter(x1, x2, y, s=10, label='data')\n",
    "    X1, X2 = np.meshgrid(np.linspace(np.amin(x1), np.amax(x1), num=2), \n",
    "                         np.linspace(np.amin(x2), np.amax(x2), num=2))\n",
    "    ax.plot_surface(X1, X2, param[0] + param[1]*X1 + param[2]*X2, \n",
    "                    label='model', alpha=0.25)\n",
    "\n",
    "interact(update, \n",
    "         rseed=IntSlider_nice(continuous_update=False), \n",
    "         N=SelSlider_nice(options=[10, 100, 1000]),\n",
    "         sigma=SelSlider_nice(options=[0.1, 0.5, 1, 2, 5, 10.]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción: Interpolación y Extrapolación\n",
    "\n",
    "Una vez que el regresor ha sido ajustado se puede usar para hacer predicciónes de la variable dependiente a partir de valores no observados de la variable independiente\n",
    "\n",
    "- Llamamos interpolación cuando predecimos dentro del rango de nuestros datos\n",
    "- Llamamos extrapolación cuando predecimos fuera del rango de nuestros datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, num=5)\n",
    "y = lambda x: -0.75*x**2 + 5*x -4\n",
    "theta = np.linalg.lstsq(np.stack((np.ones_like(x), x)).T, y(x), rcond=None)[0]\n",
    "y_hat = lambda x : np.dot(np.stack((np.ones_like(x), x)).T, theta)\n",
    "fig, ax = plt.subplots(figsize=(6, 3), tight_layout=True)\n",
    "ax.scatter(x, y(x), c='k', label='datos observados')\n",
    "ax.scatter(3, y(3), c='r', label='dato nuevo')\n",
    "x_plot = np.linspace(np.amin(x), np.amax(x))\n",
    "ax.plot(x_plot, y_hat(x_plot), 'k-', label='interpolación')\n",
    "x_plot = np.linspace(np.amax(x), 3)\n",
    "ax.plot(x_plot, y_hat(x_plot), 'k--', label='extrapolación')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos lineales en sus parámetros pero no en sus entradas\n",
    "\n",
    "Podemos generalizar la regresión lineal usando funciónes base $\\phi_j(\\cdot)$ tal que el modelo\n",
    "\n",
    "$$\n",
    "y = f_\\theta (x) = \\sum_{j=0}^N \\theta_j \\phi_j (x)\n",
    "$$\n",
    "\n",
    "#### Regresión lineal con polinomios\n",
    "\n",
    "Si usamos $\\phi_j(x) = x^j$ nos queda\n",
    "\n",
    "$$\n",
    "y = f_\\theta (x) = \\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\ldots\n",
    "$$\n",
    "\n",
    "#### Regresión lineal con sinusoides\n",
    "\n",
    "Si usamos $\\phi_j(x) = \\cos(2\\pi j x)$ nos queda\n",
    "\n",
    "$$\n",
    "y = f_\\theta (x) = \\theta_0 + \\theta_1 \\cos(2\\pi x) + \\theta_2 \\cos(4 \\pi x) + \\ldots\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2, num=100)\n",
    "y = 2*np.cos(2.0*np.pi*x) + np.sin(4.0*np.pi*x) + 0.4*np.random.randn(len(x))\n",
    "fig, ax = plt.subplots(figsize=(6, 4), tight_layout=True)\n",
    "ax.scatter(x, y)\n",
    "\n",
    "poly_basis = lambda x,N : np.vstack([x**k for k in range(N)]).T\n",
    "N = 1\n",
    "theta = np.linalg.lstsq(poly_basis(x, N), y, rcond=None)[0]\n",
    "ax.plot(x, np.dot(poly_basis(x, N), theta));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconocer un sistema lineal en sus parámetros\n",
    "\n",
    "Un sistema lineal en sus parámetros admite una factorización\n",
    "$$\n",
    "f_\\theta(\\vec x) = \\begin{pmatrix} \\phi_0(\\vec x) & \\phi_1(\\vec x) & \\ldots & \\phi_N(\\vec x) \\end{pmatrix} \\begin{pmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_N  \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "donde $\\phi_j$ son transformaciones arbitrarias de la entrada\n",
    "\n",
    "\n",
    "¿Son estos modelos lineales en sus parámetros?\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f_\\theta(x) = \\theta_0  + \\theta_1 x  \\nonumber \\\\\n",
    "y &= f_\\theta(x) = \\theta_0  + \\theta_1 x + \\theta_2 x^2 + \\theta_3 \\log(x) \\nonumber \\\\\n",
    "y &= f_\\theta(x) = \\theta_0  + \\sin(\\theta_1) x  \\nonumber \n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sistema infradeterminado\n",
    "\n",
    "Es aquel sistema que tiene más incognitas (parámetros) que ecuaciones, $N>M$\n",
    "\n",
    "Este tipo de sistema tiene infinitas soluciones\n",
    "\n",
    "#### Ejemplo: Dos puntos con polinomio de segundo orden (tres parámetros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([-2, 2])\n",
    "y = np.array([4, 4])\n",
    "fig, ax = plt.subplots(figsize=(6, 4), tight_layout=True)\n",
    "x_plot = np.linspace(-3, 3, num=100)\n",
    "thetas = np.zeros(shape=(200, 3))\n",
    "for i, a in enumerate(np.linspace(-10, 10, num=thetas.shape[0])):\n",
    "    ax.plot(x_plot, a  + (1 - a/4)*x_plot**2)\n",
    "    thetas[i:] = [a, 0, (1-a/4)]\n",
    "ax.scatter(x, y, s=100, c='k', zorder=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso $A^T A$ no es invertible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = poly_basis(x, N=3)\n",
    "display(A)\n",
    "np.linalg.inv(np.dot(A.T, A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema infradeterminado se resuelve imponiendo una restricción adicional\n",
    "\n",
    "La más típica es que el vector solución tenga norma mínima\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\| x \\|_2^2 ~\\text{s.a.}~ Ax =b\n",
    "$$\n",
    "\n",
    "que se resuelve usando $M$ multiplicadores de Lagrande\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d}{dx} \\| x\\|_2^2 + \\lambda^T (b - Ax) &= 2x - \\lambda^T A  \\nonumber \\\\\n",
    "&= 2Ax - A A^T \\lambda \\nonumber \\\\\n",
    "&= 2b - A A^T \\lambda = 0 \\nonumber \\\\\n",
    "&\\rightarrow \\lambda = 2(AA^T)^{-1}b \\nonumber \\\\\n",
    "&\\rightarrow x = \\frac{1}{2} A^T \\lambda = A^T (A A^T)^{-1} b\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde $A^T (A A^T)^{-1}$ se conoce como la pseudo-inversa \"por la derecha\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([-2, 2])\n",
    "y = np.array([4, 4])\n",
    "fig, ax = plt.subplots(figsize=(6, 4), tight_layout=True)\n",
    "x_plot = np.linspace(-3, 3, num=100)\n",
    "theta = np.dot(np.dot(A.T, np.linalg.inv(np.dot(A, A.T))), y)\n",
    "ax.plot(x_plot, np.dot(poly_basis(x_plot, N=3), theta))\n",
    "ax.scatter(x, y, s=100, c='k', zorder=10)\n",
    "display(theta)\n",
    "display(thetas[np.argmin(np.sum(thetas**2, axis=1)), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.lstsq(A, y, rcond=None)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigando nos damos cuenta que `linalg.lstsq` está basado en la función de LAPACK [`dgels`](https://www.math.utah.edu/software/lapack/lapack-d/dgels.html)\n",
    "\n",
    "dgels usa la pseudo inversa izquierda si $N<M$ o la pseudo inversa derecha si $N>M$\n",
    "\n",
    "> Se asume que la mejor solución del sistema infradeterminado es la de **mínima norma euclidiana**\n",
    "\n",
    "\n",
    "# Modelo con error cero\n",
    "\n",
    "Ajustamos nuestro regresor lineal mínimizando el **la suma de errores cuadráticos**\n",
    "\n",
    "¿Buscamos siempre un modelo de mínimo error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3, 3, num=10)\n",
    "x_plot = np.linspace(-3, 3, num=10*len(x))\n",
    "y_clean = np.poly1d([2, -4, 20]) # 2*x**2 -4*x +20\n",
    "y = y_clean(x) + 3*np.random.randn(len(x))\n",
    "fig, ax = plt.subplots(figsize=(5, 4), tight_layout=True)\n",
    "def update_plot(N_params):\n",
    "    ax.cla()\n",
    "    ax.scatter(x, y); \n",
    "    ax.plot(x_plot, y_clean(x_plot), lw=2, alpha=.5)\n",
    "    ax.set_xlabel('x'); ax.set_ylabel('y');\n",
    "    theta = np.linalg.lstsq(poly_basis(x, N=N_params), y, rcond=None)[0]\n",
    "    ax.plot(x_plot, np.dot(poly_basis(x_plot, N=N_params), theta), 'k');\n",
    "widgets.interact(update_plot, N_params=IntSlider_nice(min=1, max=20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complejidad, sobreajuste y generalización\n",
    "\n",
    "Un modelo con más parámetros es más flexible pero también más complejo\n",
    "\n",
    "> Complejidad: grados de libertad de un modelo\n",
    "\n",
    "Un exceso de flexibilidad no es bueno. Podría ocurrir que el modelo **se ajuste al ruido** \n",
    "\n",
    "> Sobreajuste: Ocurre cuando el modelo \"memoriza\" los datos\n",
    "\n",
    "Cuando esto ocurre el modelo pierde capacidad de generalización\n",
    "\n",
    "> Generalización: Capacidad de predecir adecuadamente datos no usados en el ajuste\n",
    "\n",
    "Tres maneras de evitar el sobreajuste y mejorar la capacidad de generalización\n",
    "\n",
    "- Usar modelos de baja complejidad \n",
    "- Escoger la complejidad mediante pruebas de validación \n",
    "- Usar **regularización**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validación\n",
    "\n",
    "Consiste en dividir el conjunto de datos en 3 subconjuntos\n",
    "1. Entrenamiento: Datos que se ocupan para **ajustar el modelo**\n",
    "1. Validación: Datos que se ocupan para **calibrar el modelo**\n",
    "1. Prueba: Datos que se ocupan para comparar distintos modelos y estimar el error de generalización\n",
    "\n",
    "Los datos de validación y prueba están \"escondidos\" para el modelo\n",
    "\n",
    "> Ajustamos nuestro modelo minimizando el error de entrenamiento\n",
    "\n",
    "> Seleccionamos la cantidad de parámetros minimizando el error de validación\n",
    "\n",
    "Un modelo sobreajustado tiene buen desempeño en entrenamiento y malo en validación\n",
    "\n",
    "Tipicamente se hacen **particiones aleatorias** del conjunto original\n",
    "\n",
    "Lo más importante es que las particiones sean **representativas**\n",
    "\n",
    "Retomaremos este tema más adelante..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all'); fig, ax = plt.subplots(figsize=(6, 4), tight_layout=True)\n",
    "theta = [10, -2, -0.3, 0.1]\n",
    "x = np.linspace(-5, 6, num=21); \n",
    "X = poly_basis(x, len(theta))\n",
    "y = np.dot(X, theta)\n",
    "\n",
    "rseed, sigma = 0, 1.\n",
    "np.random.seed(rseed);\n",
    "Y = y + sigma*np.random.randn(len(x))\n",
    "P = np.random.permutation(len(x))\n",
    "train_idx, valid_idx = P[:len(x)//2], P[len(x)//2:]\n",
    "\n",
    "def update_plot(ax, N):\n",
    "    ax.cla();\n",
    "    Phi = poly_basis(x, N)\n",
    "    theta_hat = np.linalg.lstsq(Phi[train_idx, :], Y[train_idx], rcond=None)[0]\n",
    "    ax.plot(x, y, 'b-', linewidth=2, label='Implícito', alpha=0.5, zorder=-100)\n",
    "    ax.scatter(x[train_idx], Y[train_idx], c='b', s=50, label='Entrenamiento')\n",
    "    ax.scatter(x[valid_idx], Y[valid_idx], c='g', s=50, label='Validación')\n",
    "    ax.vlines(x[train_idx], np.dot(Phi[train_idx, :], theta_hat), Y[train_idx], 'b')  \n",
    "    ax.vlines(x[valid_idx], np.dot(Phi[valid_idx, :], theta_hat), Y[valid_idx], 'g')     \n",
    "    x_plot = np.linspace(-5, 6, num=100);\n",
    "    ax.plot(x_plot, np.dot(poly_basis(x_plot, N), theta_hat), 'k-', linewidth=2, label='Modelo')\n",
    "    ax.set_ylim([-5, 15]); plt.legend()\n",
    "    \n",
    "interact(update_plot, ax=widgets.fixed(ax), N=IntSlider_nice(min=1, max=11));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4), tight_layout=True)\n",
    "N_values = np.arange(1, 11)\n",
    "mse = np.zeros(shape=(len(N_values), 2))\n",
    "for i, N in enumerate(N_values):\n",
    "    Phi = poly_basis(x, N)\n",
    "    theta_hat = np.linalg.lstsq(Phi[train_idx, :], Y[train_idx], rcond=None)[0]\n",
    "    mse[i, 0] = np.mean(np.power(Y[train_idx] - np.dot(Phi[train_idx, :], theta_hat), 2))\n",
    "    mse[i, 1] = np.mean(np.power(Y[valid_idx] - np.dot(Phi[valid_idx, :], theta_hat), 2))\n",
    "ax.plot(N_values, mse[:, 1], label='Validación')\n",
    "ax.plot(N_values, mse[:, 0], label='Entrenamiento')\n",
    "idx_best = np.argmin(mse[:, 1])\n",
    "ax.scatter(N_values[idx_best], mse[idx_best, 1])\n",
    "plt.legend()\n",
    "ax.set_ylim([1e-4, 1e+4])\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularización\n",
    "\n",
    "Consiste en agregar una penalización adicional al problema \n",
    "\n",
    "El ejemplo clásico es pedir que la solución tenga norma mínima\n",
    "\n",
    "$$\n",
    "\\min_x \\|Ax-b\\|_2^2 + \\lambda \\|x\\|_2^2\n",
    "$$\n",
    "\n",
    "En este caso la solución es\n",
    "\n",
    "$$\n",
    "\\hat x = (A^T A + \\lambda I)^{-1} A^T b\n",
    "$$\n",
    "\n",
    "que se conoce como **ridge regression** o **regularización de Tikhonov**\n",
    "\n",
    "$\\lambda$ es un hiper-parámetro del modelo y debe ser escogido por el usuario (usando validación)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all'); fig, ax = plt.subplots(figsize=(7, 4))\n",
    "x = np.linspace(-5, 5, num=20); \n",
    "x_plot = np.linspace(-5, 5, num=200); \n",
    "model = np.sin(x)*x + 0.1*x**2\n",
    "\n",
    "def update(N, lamb, sigma, rseed):\n",
    "    np.random.seed(rseed); \n",
    "    y = model + sigma*np.random.randn(len(x))\n",
    "    y = (y - np.mean(y))/np.sqrt(np.sum(y**2))\n",
    "    X = poly_basis(x, N); X_plot = poly_basis(x_plot, N)\n",
    "    Lamb = lamb*np.identity(N); Lamb[0, 0] = 0\n",
    "    theta = np.linalg.solve(np.dot(X.T, X) + Lamb, np.dot(X.T, y))\n",
    "    ax.cla(); \n",
    "    ax.plot(x_plot , np.dot(X_plot, theta), 'k-', linewidth=2, label='Modelo')\n",
    "    ax.scatter(x, y, c='b', s=30, label='Datos', zorder=100); plt.legend()\n",
    "\n",
    "    \n",
    "interact(update, rseed=IntSlider_nice(min=1, max=10), \n",
    "         N=SelSlider_nice(options=[1, 2, 3, 5, 7, 10, 15, 17, 20]), \n",
    "         sigma=SelSlider_nice(options=[0.1, 1, 2, 5]),\n",
    "         lamb=SelSlider_nice(options=[0.0, 0.1, 1., 10., 1e+2, 1e+3, 1e+4, 1e+5, 1e+6]));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opciones de más alto nivel para hacer regresión lineal\n",
    "\n",
    "- `scipy.stats.linregress`\n",
    "- [`sklearn.linear_model.LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "#from sklearn.preprocessing import PolynomialFeatures\n",
    "#from sklearn.pipeline import make_pipeline\n",
    "\n",
    "plt.close('all'); fig, ax = plt.subplots(figsize=(7, 4))\n",
    "x = np.linspace(-5, 5, num=20); \n",
    "x_plot = np.linspace(-5, 5, num=200); \n",
    "model = np.sin(x)*x + 0.1*x**2\n",
    "\n",
    "def update(N, lamb, sigma, rseed):\n",
    "    np.random.seed(rseed); \n",
    "    y = model + sigma*np.random.randn(len(x))\n",
    "    X = poly_basis(x, N); X_plot = poly_basis(x_plot, N)\n",
    "    #regressor = make_pipeline(PolynomialFeatures(N-1), \n",
    "    #                          Ridge(normalize=True, fit_intercept=True, alpha=lamb))\n",
    "    regressor = Ridge(normalize=True, fit_intercept=True, alpha=lamb)\n",
    "    regressor.fit(X, y)\n",
    "    ax.cla(); \n",
    "    ax.plot(x_plot , regressor.predict(X_plot), 'k-', linewidth=2, label='Modelo')\n",
    "    ax.scatter(x, y, c='b', s=30, label='Datos', zorder=100); plt.legend()\n",
    "\n",
    "    \n",
    "interact(update, rseed=IntSlider_nice(min=1, max=10), \n",
    "         N=SelSlider_nice(options=[1, 2, 3, 5, 7, 10, 15, 17, 20]), \n",
    "         sigma=SelSlider_nice(options=[0.1, 1, 2, 5]),\n",
    "         lamb=SelSlider_nice(options=[0.0, 1e-4, 1e-3, 1e-2, 1e-1, 1., 10.]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad práctica: Predicción de calidad de aire\n",
    "\n",
    "Considere el siguiente data-set\n",
    "https://archive.ics.uci.edu/ml/datasets/Air+Quality\n",
    "\n",
    "- Importe y \"parsee\" el dataset usando pandas\n",
    "- Entrene un predictor para la concentración de CO y otro para el Benzeno (C6H6)\n",
    "- Use la primera semana de datos para ajustar y la siguiente semana para probar\n",
    "- Observe los datos ¿Qué tipo de modelo es apropiado en este caso? \n",
    "- Encuentre la cantidad de parámetros que minimiza el error de validación\n",
    "\n",
    "Ojo: Un valor de \"-200\" es un *missing value*\n",
    "\n",
    "Documentación pandas timestamp: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget -c https://archive.ics.uci.edu/ml/machine-learning-databases/00360/AirQualityUCI.zip\n",
    "unzip AirQualityUCI.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"AirQualityUCI.csv\", sep=';', decimal=',')\n",
    "df = df[df.columns[:-2]] \n",
    "df.dropna(inplace=True)\n",
    "df[\"Date_time\"] = pd.to_datetime(df['Date'] + ' ' + df['Time'], format=\"%d/%m/%Y %H.%M.%S\")\n",
    "df.drop([\"Date\", \"Time\"], axis=1, inplace=True)\n",
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[(df[\"Date_time\"] < pd.to_datetime(\"03/24/2004\")) & (df[\"CO(GT)\"] > -200)][[\"Date_time\", \"CO(GT)\"]].plot(x=0, y=1)\n",
    "time, data = df.loc[(df[\"Date_time\"] < pd.to_datetime(\"03/24/2004 18:00:00\")) & (df[\"CO(GT)\"] > -200)][[\"Date_time\", \"CO(GT)\"]].values.T\n",
    "data = data.astype('float32')\n",
    "time_float = np.array([t.timestamp()/(24*3600) - time[0].timestamp()/(24*3600) for t in time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "mask = time_float <= 7\n",
    "ax.plot(time_float[mask], data[mask])\n",
    "\n",
    "def build_phi(time, freqs=[1]):\n",
    "    phi_cos = np.stack([np.cos(2*np.pi*time*f) for f in [0]+freqs])\n",
    "    phi_sin = np.stack([np.sin(2*np.pi*time*f) for f in freqs])\n",
    "    return np.concatenate((phi_cos, phi_sin), axis=0).T\n",
    "\n",
    "X_lstsq = build_phi(time_float[mask])\n",
    "theta, MSE, rank, singval = np.linalg.lstsq(X_lstsq, data[mask], rcond=None)\n",
    "ax.plot(time_float[mask], np.dot(X_lstsq, theta))\n",
    "mask = time_float > 7\n",
    "ax.plot(time_float[mask], data[mask])\n",
    "X_lstsq = build_phi(time_float[mask])\n",
    "mse = np.mean(np.power(np.dot(X_lstsq, theta) - data[mask], 2))\n",
    "display(\"Mean Square Error:\"+str(mse))\n",
    "ax.plot(time_float[mask], np.dot(X_lstsq, theta));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm AirQualityUCI*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUTURO: outliers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
