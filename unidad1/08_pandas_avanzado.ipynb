{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulación de datos con [*pandas*](https://pandas.pydata.org/) <a class=\"tocSkip\">\n",
    "\n",
    "En este clase veremos \n",
    "- como crear dataframes a partir de datos de distintas fuentes \n",
    "- funciones avanzadas de manipulación de dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(\"Versión de pandas \"+pd.__version__)\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de archivos CSV\n",
    "\n",
    "Un archivo  CSV (Comma-Separated Values) es una tabla en formato texto plano cuyas columnas están separadas por comas\n",
    "\n",
    "Descarguemos la base de datos \"Dow Jones Index\" del repositorio UCI\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Dow+Jones+Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget -c https://archive.ics.uci.edu/ml/machine-learning-databases/00312/dow_jones_index.zip\n",
    "unzip -o dow_jones_index.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizemos el archivo csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head dow_jones_index.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provee la función [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) para importar tablas en formato texto plano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dow_jones_index.data\", sep=',', header=0, index_col='stock')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas columnas se han guardado como strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[\"date\"][0],\n",
    "        df[\"open\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing: Selección manual de formato de dato"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora hemos dejado que pandas asigne los tipos de dato de manera automática\n",
    "\n",
    "La función `read_csv` tiene un argumento `converters` que recibe un diccionar de funciones\n",
    "\n",
    "Esto puede usarse para *parsear* manualmente las columnas que no se importaron automaticamente como deseabamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = dict.fromkeys(['open', 'close', 'high', 'low', \n",
    "                      'next_weeks_open', 'next_weeks_close'], lambda x: float(x.strip(\"$\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas fue diseñado para analizar series de tiempo e incorpora la función [`to_datetime()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html) que toma un string y retorna un `Timestamp`\n",
    "\n",
    "Por defecto el formato se infiere, pero puede forzarce usando el argumento `format`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.to_datetime(\"1/5/2018\"),\n",
    "        pd.to_datetime(\"1st of May of 2018\"),\n",
    "        pd.to_datetime(\"May/1/2018\"),\n",
    "        pd.to_datetime(\"2018\"),\n",
    "        pd.to_datetime(\"14:45\"),\n",
    "        pd.to_datetime(\"May/1/2018 14:45\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para facilitarnos la vida `read_csv` tiene un argumento `parse_dates` que recibe una lista de enteros especificando las columnas que queremos convertir a fechas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dow_jones_index.data\", sep=',', header=0, index_col='stock', \n",
    "                 converters=conv, parse_dates=[2])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los tipos de la nueva tabla son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que los tiempos tiene formato *timestamp* podemos usarlos como índice\n",
    "\n",
    "Esto nos permite recuperar rapidamente todos los eventos dentro de un intervalo de tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dow_jones_index.data\", sep=',', header=0, index_col='date', \n",
    "                 converters=conv, parse_dates=[2])\n",
    "\n",
    "df[df[\"stock\"] == \"AA\"].loc[\"2011-02-01\":\"2011-03-12\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matplotlib tiene funciones para parsear datos temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as md\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "sub_df = df[df[\"stock\"] == \"AA\"]\n",
    "for x, o, c in zip(sub_df.index.values, sub_df['open'].values, sub_df['close'].values):\n",
    "    ax.arrow(x=md.date2num(x), y=o, dx=0, dy=c-o, head_width=3, head_length=0.1, fc='k', ec='k')\n",
    "ax.fill_between(sub_df.index.values, sub_df['low'].values, sub_df['high'].values, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de archivos excel\n",
    "\n",
    "- Pandas provee la función [`read_excel`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html)\n",
    "\n",
    "- Requisito adicional: [python-xlrd](https://github.com/python-excel/xlrd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget -c http://www.censo2017.cl/wp-content/uploads/2017/12/Cantidad-de-Viviendas-por-Tipo.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Cantidad-de-Viviendas-por-Tipo.xlsx\", sheet_name=1, \n",
    "                   usecols=list(range(1, 20)), header=1, index_col='ORDEN')\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.drop(0, inplace=True)\n",
    "display(df.head())\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podriamos querer obtener los valores totales de la Provincia de Valdivia: **reducción suma**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_mask = df.columns[4:-1]\n",
    "display(col_mask)\n",
    "row_mask = df[\"NOMBRE PROVINCIA\"] == \"VALDIVIA\"\n",
    "display(df.loc[row_mask].head())\n",
    "df.loc[row_mask, col_mask].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cambio de índice\n",
    "\n",
    "Podemos usar las funciones [reset_index](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html) y [set_index](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_index.html#pandas.DataFrame.set_index) para modificar el índice del dataframe a nuestra conveniencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df = df.set_index(\"NOMBRE PROVINCIA\")\n",
    "display(df.head())\n",
    "df.loc[\"VALDIVIA\", col_mask].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df = df.set_index(\"ORDEN\")\n",
    "%timeit -n20 df.loc[df[\"NOMBRE PROVINCIA\"] == \"VALDIVIA\", col_mask].sum()\n",
    "df = df.reset_index()\n",
    "df = df.set_index(\"NOMBRE PROVINCIA\")\n",
    "%timeit -n20 df.loc[\"VALDIVIA\", col_mask].sum()\n",
    "df = df.reset_index()\n",
    "df = df.set_index(\"ORDEN\")"
   ]
  },
  {
   "attachments": {
    "groupby.svg": {
     "image/svg+xml": [
      "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"
     ]
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby: Reducciones condicionales\n",
    "\n",
    "En el caso anterior podemos reducir de forma separada para cada región o provincia sin cambiar índices\n",
    "\n",
    "La función [`groupby`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) permite hacer una reducción condicional a una etiqueta\n",
    "\n",
    "Podemos imaginar que la función `groupby` es una [secuencia](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html) como la siguiente:\n",
    "![groupby.svg](attachment:groupby.svg)\n",
    "\n",
    "Donde\n",
    "- *Split*: divide los datos según una **llave**\n",
    "- *Apply*: Realiza una función sobre cada grupo: reducción, transformación, filtrado\n",
    "- *Combine*: Mezcla el resultado en un nuevo dataframe donde la **llave** se convierte en el índice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Cantidad-de-Viviendas-por-Tipo.xlsx\", sheet_name=1, \n",
    "                   usecols=list(range(1, 20)), header=1, index_col='ORDEN')\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.drop(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_redu = df.groupby(\"NOMBRE REGIÓN\").sum()\n",
    "display(df_redu.head())\n",
    "fig, ax = plt.subplots(figsize=(6, 7), tight_layout=True)\n",
    "df_redu.plot(ax=ax, y=0, kind='bar', logy=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`groupby` puede usarse como iterador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for (region, sub_df) in df.groupby('NOMBRE REGIÓN'):\n",
    "    display(region, sub_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que no estamos limitados a las reducciones definidas\n",
    "\n",
    "Podemos usar los atributos de `groupby` para obtener más funcionalidad\n",
    "- `aggregate` : Operación de reducción\n",
    "- `filter` : Operación de eliminación de filas (drop)\n",
    "- `transform` : Operación de modificación columna a columna\n",
    "- `apply`: Aplica una función arbitraria "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df.columns[6:]\n",
    "print(mask)\n",
    "\n",
    "# Las funciones de reducción deben recibir un arreglo y retornar un valor\n",
    "def mi_reduccion(x):\n",
    "    N = len(x)\n",
    "    return np.sqrt(np.sum((x - np.mean(x))**2)/(N-1))\n",
    "\n",
    "# Argumento lista de funciones: Cada función se aplica a todas las columnas\n",
    "display(df.groupby(\"NOMBRE REGIÓN\")[mask].aggregate([mi_reduccion, np.std]).head())\n",
    "# Argumento diccionario de funciones: una función distinta por columna\n",
    "display(df.groupby(\"NOMBRE REGIÓN\").aggregate({mask[0]: np.std, \n",
    "                                               mask[1]: mi_reduccion}).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mask[0], mask[-1])\n",
    "\n",
    "def mi_filtro(x):\n",
    "    #Regiones donde en promedio las comunas tengan una proporcion de viviendas ocupadas(presentes)/total mayor a 85%\n",
    "    return np.mean(x[mask[0]]/x[mask[-1]]) > 0.8\n",
    "    # Regiones donde la proporción de viviendas ocupadas(presentes) / total sea mayor a 85%\n",
    "    #return np.sum(x[mask[0]])/np.sum(x[mask[-1]]) > 0.8\n",
    "\n",
    "# El filtro debe establecer una condición sobre el grupo completo\n",
    "# En este caso retorna df menos las comunas de las regiones con menos de 500.000 viviendas\n",
    "\n",
    "sub_df = df.groupby(\"NOMBRE REGIÓN\").filter(mi_filtro)\n",
    "display(sub_df[\"NOMBRE REGIÓN\"].unique(),\n",
    "        sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mi_transformación(x):\n",
    "    if x.dtype == np.float:\n",
    "        return (x - x.mean())/x.std()\n",
    "    else:\n",
    "        return x \n",
    "    \n",
    "def mi_transformación2(x):\n",
    "    return (x - x.mean())/x.std()\n",
    "\n",
    "# La transformación opera columna por columna\n",
    "# La transformación debe retornar un resultado que es del mismo tamaño de la entrada\n",
    "\n",
    "display(df.groupby(\"NOMBRE REGIÓN\").transform(mi_transformación), \n",
    "        df.groupby(\"NOMBRE REGIÓN\")[mask].transform(mi_transformación2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mi_funcion(x):\n",
    "    col = 'Viviendas Particulares Ocupadas con Moradores Presentes'\n",
    "    x[col] -= x[col].mean()\n",
    "    return x\n",
    "\n",
    "\n",
    "df.groupby(\"NOMBRE REGIÓN\").apply(mi_funcion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting\n",
    "\n",
    "Podemos usar más de una etiqueta para hacer el splitting de `groupby`\n",
    "\n",
    "Podemos también usar una función, una lista, diccionario o dataframe\n",
    "\n",
    "Refierase a la [documentación](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html) para más detalles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"NOMBRE REGIÓN\", \"NOMBRE PROVINCIA\"]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi índices\n",
    "\n",
    "Hemos notado que la tabla tiene estructura jerárquica: REGION, PROVINCIA, COMUNA\n",
    "\n",
    "Podemos usar un multi-índice en pandas para manipular mejor esta tabla\n",
    "\n",
    "Podemos crear un multi-índice usando la modulo `MultiIndex` y sus funciones `from_array`, `from_frame` y `from_tuple`\n",
    "\n",
    "Luego podemos aplicarlo con la función `set_index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set_index acepta una lista con nombres de columna:\n",
    "df.reset_index()\n",
    "df = df.set_index([\"NOMBRE REGIÓN\", \"NOMBRE PROVINCIA\"])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexando y haciendo slicing con dataframes multi-indexados\n",
    "\n",
    "Usamos una tupla para especificar los índices primario y secundario\n",
    "\n",
    "Se usa el objeto [`IndexSlice`](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.IndexSlice.html) para generar slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "\n",
    "display(df.loc[\"LOS RÍOS\"],\n",
    "        df.loc[(\"LOS LAGOS\", \"OSORNO\")],\n",
    "        df.loc[idx[:, \"VALDIVIA\"], :],\n",
    "        df.loc[(\"LOS RÍOS\", \"RANCO\"), \"Viviendas Particulares Ocupadas con Moradores Presentes\":])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La función `eval` y la función `query` \n",
    "\n",
    "[`eval`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.eval.html) y [`query`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html) son atributos de DataFrame que permiten evaluar una expresión arbitraria o hacer consultas (filtro) sobre las columnas del dataframe, respectivamente\n",
    "\n",
    "Están basadas en [`numexpr`](https://github.com/pydata/numexpr) que es un evaluador de expresiones numéricas acelerado para ndarray (rendimiento casi C)\n",
    "\n",
    "`numexpr` acepta un string con una expresión estilo numpy, la evalua y returna el resultado\n",
    "\n",
    "Si los arreglos son grandes ganamos en velocidad y en memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numexpr as ne\n",
    "\n",
    "A = np.arange(100000).reshape(1000, 100)\n",
    "\n",
    "# NumPy es más lento ya que evalua y guarda cada paso\n",
    "%timeit -n10 np.tanh(-A**2) > np.exp(np.cos(A)/2)\n",
    "%timeit -n10 ne.evaluate(\"tanh(-A**2) > exp(cos(A)/2)\")\n",
    "\n",
    "b1 = np.tanh(-A**2) > np.exp(np.cos(A)/2)\n",
    "b2 = ne.evaluate(\"tanh(-A**2) > exp(cos(A)/2)\")\n",
    "np.allclose(b1, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OJO:** Nuestras columnas no pueden tener espacios en sus nombres para poder usar `query`/`eval`\n",
    "\n",
    "### Renombrando las columnas\n",
    "\n",
    "Podemos \n",
    "\n",
    "- Usar el atributo `rename` y especificar los nuevos nombres uno a uno \n",
    "- Aplicar operaciones de string al atributo `columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Cantidad-de-Viviendas-por-Tipo.xlsx\", sheet_name=1, \n",
    "                   usecols=list(range(1, 20)), header=1, index_col='ORDEN')\n",
    "df.dropna(inplace=True)\n",
    "df.drop(0, inplace=True)\n",
    "# df.rename(columns={'NOMBRE PROVINCIA': 'NOMBRE_PROVINCIA'}, inplace=True)\n",
    "df.columns = df.columns.str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de eval y query\n",
    "\n",
    "Hacemos operaciones o consultas sobre las columnas usando su etiqueta\n",
    "\n",
    "Para `query` podemos juntar varias consultas con `and` y `or`\n",
    "\n",
    "Podemos llamar variables externas anteponiendo un `@`\n",
    "\n",
    "Al igual que con numexpr las operaciones intermedias no se guardan en memoria\n",
    "\n",
    "- Opinión objetiva: Si el dataframe es grande ganamos en velocidad en uso de memoria\n",
    "- Opinion subjetiva: En general ganamos en legibilidad\n",
    "\n",
    "Forma tradicional versus `eval`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df['Viviendas_Particulares_Ocupadas_con_Moradores_Presentes']/df['TOTAL_VIVIENDAS']\n",
    "b = df.eval('Viviendas_Particulares_Ocupadas_con_Moradores_Presentes/TOTAL_VIVIENDAS')\n",
    "np.allclose(a, b)\n",
    "display(b.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos evaluar y guardar el resultado directamente en el dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.eval('Proporcion_encuestas_vs_total = Viviendas_Particulares_Ocupadas_con_Moradores_Presentes/TOTAL_VIVIENDAS', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrando con `query`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = 0.75\n",
    "df.query('NOMBRE_PROVINCIA == \"VALDIVIA\" \\\n",
    "and Viviendas_Particulares_Ocupadas_con_Moradores_Presentes/TOTAL_VIVIENDAS > @percentage').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de bases de datos SQL\n",
    "\n",
    "Pandas puede usarse para leer y hacer consultas a una base de datos en formato SQL\n",
    "\n",
    "Primero vamos a crear una base de datos e insertar un dataframe como tabla\n",
    "\n",
    "Usaremos [sqlite3](https://docs.python.org/3/library/sqlite3.html) que es parte de la librería estándar de Python\n",
    "\n",
    "- sqlite permite conectar a una base de datos local: RAM, disco, o disco externo montado\n",
    "- sqlite no está diseñado para soportar múltiples usuarios conectados a una misma base de datos\n",
    "- Alternativas: [SQL Alchemy](https://www.sqlalchemy.org/), [PostgreSQL+Python](http://initd.org/psycopg/), [Peewee](http://docs.peewee-orm.com/en/latest/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "# Abrimos una conexión\n",
    "conn = sqlite3.connect(\":memory:\")\n",
    "# La palabra clave :memory: corresponde a una base de datos en memoria RAM\n",
    "df.to_sql(\"censo_viviendas\", conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar [`read_sql_query()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql_query.html) para hacer una consulta y retornar un dataframe\n",
    "\n",
    "Por ejemplo si queremos toda la tabla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_string = \"SELECT * FROM censo_viviendas limit 10\"\n",
    "print(sql_string)\n",
    "pd.read_sql_query(sql_string, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O si quieremos un subconjunto específico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_string = \"SELECT [{0}], [{1}] FROM censo_viviendas WHERE [{2}] = 'VALDIVIA' limit 5\".format(\"Viviendas_Particulares_Ocupadas_con_Moradores_Presentes\", \n",
    "                                                                                                \"NOMBRE_COMUNA\",\n",
    "                                                                                                \"NOMBRE_PROVINCIA\")\n",
    "print(sql_string)\n",
    "pd.read_sql_query(sql_string, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cerramos la conexión a la base de datos\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar y leer una tabla en formato JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar el atributo [`to_json`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_json.html) para convertir un dataframe a este formato\n",
    "\n",
    "El keyword `orient` nos permite seleccionar como organizar el scheme del json\n",
    "\n",
    "Las opciones son `columns` (por defecto), `table`, `values`, `index`, `split` y `records`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"pandas.json\", orient='table')\n",
    "\n",
    "!head -c 200 pandas.json\n",
    "print(\"\")\n",
    "!ls -lah pandas.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tenemos que usar la misma orientación con la que lo guardamos\n",
    "pd.read_json(\"pandas.json\", orient='table').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar y leer una tabla en formato HDF5\n",
    "\n",
    "Podemos usar el atributo `to_hdf` para convertir nuestra tabla a formato HDF5\n",
    "\n",
    "Para acceso de más bajo nivel podemos usar la clase `HDFStore`\n",
    "\n",
    "Para lectura podemos usar la función `read_hdf`\n",
    "\n",
    "OJO: Requiere Pytables mayor a 3.5: https://github.com/PyTables/PyTables/issues/719"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.to_hdf(\"pandas_hdf.h5\", key='excel', mode='w')\n",
    "\n",
    "mi_tabla_recuperada = pd.read_hdf(\"pandas_hdf.h5\", key='/excel', mode='r')\n",
    "display(mi_tabla_recuperada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File(\"pandas_hdf.h5\", mode=\"r\") as f:\n",
    "    print(f[\"excel\"].keys())\n",
    "    print(f[\"excel\"]['block0_items'][()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
